# Evaluation

## Overview

This directory contains the evaluation scripts used in our paper:

> **Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization**

The code here supports model assessment on various code generation benchmarks to analyze memorization, generalization, and information bottleneck effects.

A portion of the evaluation utilities is adapted from [**VeriCoder**](https://github.com/Anjiang-Wei/VeriCoder/tree/986a3895dbc42351f4c62d8c269974cd5538e17a), which provides baseline implementations and evaluation pipelines for Verilog code generation tasks.

---
## ðŸš© Roadmap

- [ ] **Multiple GPU Version**: multeple GPU version is developing in orfer to speedup evaluation 

## Citation

If you use this evaluation framework, please cite both our paper and the original VeriCoder repository.
